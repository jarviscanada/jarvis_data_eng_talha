{"paragraphs":[{"text":"%md\n### How to interact with Spark\n\nTo start a Spark job (either single JVM or distributed mode), we can simply execute `bin/spark-shell` cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)\n\n> Compare SparkContext and Spark Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\n\nAlternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (`spark`) and a SparkContext (`sc`) for you.\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to interact with Spark</h3>\n<p>To start a Spark job (either single JVM or distributed mode), we can simply execute <code>bin/spark-shell</code> cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)</p>\n<blockquote>\n  <p>Compare SparkContext and Spark <a href=\"Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</blockquote>\n<p>Alternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (<code>spark</code>) and a SparkContext (<code>sc</code>) for you.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085683_1653053811","id":"20190921-014743_1530188134","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:161"},{"text":"%spark\n\n//Spark session and sparkContext are loaded automatically\nprintln(spark.version.to)\nprintln(spark)\n\n//The following two lines point to the same SparkContext@2a695829 where @2a695829 is the memory address\nprintln(spark.sparkContext)\nprintln(sc)\n","user":"anonymous","dateUpdated":"2020-03-18T02:01:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.3.4\norg.apache.spark.sql.SparkSession@12c03376\norg.apache.spark.SparkContext@649714b\norg.apache.spark.SparkContext@649714b\n"}]},"apps":[],"jobName":"paragraph_1584230085686_1744096637","id":"20190921-013657_404311467","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:01:35+0000","dateFinished":"2020-03-18T02:01:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"%md\nSparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with Dataframe and Dataset APIs. All the functionality available with sparkContext are also available in sparkSession. \n\nSparkSession was introduced in Spark 2.0 to make it easy for the developers so we don’t have worry about different contexts and to streamline the access to different contexts. By having access to SparkSession, we automatically have access to the SparkContext.","user":"anonymous","dateUpdated":"2020-03-17T22:04:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with Dataframe and Dataset APIs. All the functionality available with sparkContext are also available in sparkSession. </p>\n<p>SparkSession was introduced in Spark 2.0 to make it easy for the developers so we don’t have worry about different contexts and to streamline the access to different contexts. By having access to SparkSession, we automatically have access to the SparkContext.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584482391976_-981802426","id":"20200317-215951_1846798512","dateCreated":"2020-03-17T21:59:51+0000","dateStarted":"2020-03-17T22:04:47+0000","dateFinished":"2020-03-17T22:04:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"text":"%md\n### Creating RDDs from Scala collections\n\nWe can use `sc.parallelize` method to create RDDs from Scala collections\n(Note: `parallelize` is not available in the `SparkSession`. However, you can use `SparkSession.sparkContext.parallelize` instead)\n\nhttps://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Scala collections</h3>\n<p>We can use <code>sc.parallelize</code> method to create RDDs from Scala collections<br/>(Note: <code>parallelize</code> is not available in the <code>SparkSession</code>. However, you can use <code>SparkSession.sparkContext.parallelize</code> instead)</p>\n<p><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085687_538580401","id":"20190921-022812_325072599","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"text":"%spark\n//Create RDDs from Scala collections\nval lsRdd = sc.parallelize(List(1,2,3,4,5))\n\n//number of items in lsRdd\nval count = lsRdd.count\n\n//first element in lsRdd\nval firstE = lsRdd.first\n\n//Number of partitions\nval partitionsNum = lsRdd.partitions.length\n\n//Manipulating lsRDD\nval dupRdd = lsRdd.flatMap(i => List.fill(i)(i))\nval dupArray = dupRdd.collect\nval evens = dupRdd.filter(_%2 == 0).collect","user":"anonymous","dateUpdated":"2020-03-18T02:01:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lsRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[190] at parallelize at <console>:29\ncount: Long = 5\nfirstE: Int = 1\npartitionsNum: Int = 2\ndupRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[191] at flatMap at <console>:41\ndupArray: Array[Int] = Array(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)\nevens: Array[Int] = Array(2, 2, 4, 4, 4, 4)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=84","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=85","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=86","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=87"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085687_1533255703","id":"20190921-020350_225494359","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:01:40+0000","dateFinished":"2020-03-18T02:01:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:165"},{"text":"%md","user":"anonymous","dateUpdated":"2020-03-17T22:05:13+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085687_-1170573995","id":"20190922-220230_613999600","dateCreated":"2020-03-14T23:54:45+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:166"},{"text":"%md\n### Creating RDDs from Data source\n\n- `ssh` to dataproc master node\n- Download `online-retail-dataset.txt` dataset [link](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv)\n- Upload `online-retail-dataset.txt` to a HDFS location (e.g. hdfs dfs -put ...)\n- Inspect the dataset using spark RDD (see below Spark code)\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions","user":"anonymous","dateUpdated":"2020-03-17T22:09:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Data source</h3>\n<ul>\n  <li><code>ssh</code> to dataproc master node</li>\n  <li>Download <code>online-retail-dataset.txt</code> dataset <a href=\"https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv\">link</a></li>\n  <li>Upload <code>online-retail-dataset.txt</code> to a HDFS location (e.g. hdfs dfs -put &hellip;)</li>\n  <li>Inspect the dataset using spark RDD (see below Spark code)</li>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085687_1443361285","id":"20190920-182511_1653833929","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-17T22:09:51+0000","dateFinished":"2020-03-17T22:09:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:167"},{"text":"%spark\nval retailRDD = sc.textFile(\"hdfs:///user/talha746/datasets/online_retail/online-retail-dataset.txt\")\n\n//count the number of elements in the RDD\nval retailRDDcount = retailRDD.count\n\n//understand what each element looks like in RDD\nval firstE = retailRDD.first()\n\nval sample = retailRDD.takeSample(false, 4, 1)","user":"anonymous","dateUpdated":"2020-03-18T02:01:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":244.4,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/talha746/datasets/online_retail/online-retail-dataset.txt MapPartitionsRDD[194] at textFile at <console>:28\nretailRDDcount: Long = 541910\nfirstE: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\nsample: Array[String] = Array(561322,21264,WHITE GOOSE FEATHER TREE 60CM ,12,7/26/2011 13:03,1.95,15203,United Kingdom, 555604,85099B,JUMBO BAG RED RETROSPOT,10,6/6/2011 11:37,2.08,16612,United Kingdom, 562439,22991,GIRAFFE WOODEN RULER,432,8/4/2011 18:06,1.65,12931,United Kingdom, 547669,21930,JUMBO STORAGE BAG SKULLS,2,3/24/2011 13:37,1.95,12748,United Kingdom)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=88","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=89","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=90","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=91"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085688_723475596","id":"20190920-182724_1961848616","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:01:54+0000","dateFinished":"2020-03-18T02:01:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:168"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085688_760772797","id":"20190922-220256_1973670371","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:169"},{"text":"%md\n### CSV format issue\n- Right now we are parsing by splitting the fields by commas. This leads to being extra columns as we dont want to split at every single comma. \n\n- We can solve this by simply removing commas between the double quotes and then remove all the double quotes instead of having a more complex mapping function.","user":"anonymous","dateUpdated":"2020-03-17T22:20:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>CSV format issue</h3>\n<ul>\n  <li>\n  <p>Right now we are parsing by splitting the fields by commas. This leads to being extra columns as we dont want to split at every single comma.</p></li>\n  <li>\n  <p>We can solve this by simply removing commas between the double quotes and then remove all the double quotes instead of having a more complex mapping function.</p></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085688_1702472010","id":"20190921-023538_989684097","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-17T22:20:06+0000","dateFinished":"2020-03-17T22:20:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:170"},{"text":"%spark\n//Check CSV format\nval splitRdd = retailRDD.map(s => s.split(\",\"))\n\n//Some lines have more than 8 column which indicates a format issue\nval samples = splitRdd.map(arr => arr.length).takeSample(false,15, 11)\n\n//find out how many lines have more than 8 cols\nval lenArrRdd = splitRdd.map(arr => (arr.length, arr))\nlenArrRdd.filter(_._1 != 8).take(3).foreach({case(count, columns) => println(count + \":\" + columns.mkString(\"||\"))})\n","user":"anonymous","dateUpdated":"2020-03-18T02:02:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"9:536381||82567||\"AIRLINE LOUNGE||METAL SIGN\"||2||12/1/2010 9:41||2.1||15311||United Kingdom\n9:536394||21506||\"FANCY FONT BIRTHDAY CARD|| \"||24||12/1/2010 10:39||0.42||13408||United Kingdom\n9:536520||22760||\"TRAY|| BREAKFAST IN BED\"||1||12/1/2010 12:43||12.75||14729||United Kingdom\nsplitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[196] at map at <console>:34\nsamples: Array[Int] = Array(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8)\nlenArrRdd: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[199] at map at <console>:40\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=92","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=93","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=94"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085689_-726040297","id":"20190921-023311_1509488233","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:02:06+0000","dateFinished":"2020-03-18T02:02:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:171"},{"text":"%md\n### Pre-process dataset\n\nWe need to deal with fields that contain commas. e.g. `123,\"seond, field\",\"third field\"`. We have seen this csv format issue in Hive, and we solved it using `OpenCSV` SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.\n\n- Removing commas in `Description` field (e.g. \"Apple, Inc\" => \"Apple Inc\")<br>`awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' online-retail-dataset.txt`\n- Remove all double double quotes<br>`sed 's/\"//g' online-retail-dataset.txt`\n- output file: `online-retail-dataset_clean.txt`\n\n### Move file to HDFS\nMove `online-retail-dataset_clean.txt` to HDFS using `hdfs dfs -put ..`\n\n### Spark RDD Cache\n- https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type","user":"anonymous","dateUpdated":"2020-03-17T22:22:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pre-process dataset</h3>\n<p>We need to deal with fields that contain commas. e.g. <code>123,&quot;seond, field&quot;,&quot;third field&quot;</code>. We have seen this csv format issue in Hive, and we solved it using <code>OpenCSV</code> SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.</p>\n<ul>\n  <li>Removing commas in <code>Description</code> field (e.g. &ldquo;Apple, Inc&rdquo; =&gt; &ldquo;Apple Inc&rdquo;)<br><code>awk -F&#39;&quot;&#39; -v OFS=&#39;&#39; &#39;{ for (i=2; i&lt;=NF; i+=2) gsub(&quot;,&quot;, &quot;&quot;, $i) } 1&#39; online-retail-dataset.txt</code></li>\n  <li>Remove all double double quotes<br><code>sed &#39;s/&quot;//g&#39; online-retail-dataset.txt</code></li>\n  <li>output file: <code>online-retail-dataset_clean.txt</code></li>\n</ul>\n<h3>Move file to HDFS</h3>\n<p>Move <code>online-retail-dataset_clean.txt</code> to HDFS using <code>hdfs dfs -put ..</code></p>\n<h3>Spark RDD Cache</h3>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085689_42350501","id":"20190519-113048_765206384","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-17T22:22:14+0000","dateFinished":"2020-03-17T22:22:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:172"},{"text":"//Load csv file\n//Lazy evaluation\n//val datasetDir = \"/home/centos/dev/jrvs/bootcamp/hadoop/datasets\"\nval filePath = \"hdfs:///user/talha746/datasets/online_retail/online-retail-dataset_clean.txt\"\nval retailRDD = sc.textFile(filePath)\n\n//RDD action triggers evaluation (in this case count is an action)\nval count = retailRDD.count\n\n//tip: Use tab key to auto-complete\nval sample3 = retailRDD.takeSample(false, 3, 22)\n\n//Make sure every row has exactly 8 columns\nval longRow = retailRDD.filter(row => row.split(\",\").length != 8 ).count\n//If this is zero then we have solved this issue\n\n//Cache RDD since it will be accessed frequently\nretailRDD.cache","user":"anonymous","dateUpdated":"2020-03-18T02:02:14+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filePath: String = hdfs:///user/talha746/datasets/online_retail/online-retail-dataset_clean.txt\nretailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/talha746/datasets/online_retail/online-retail-dataset_clean.txt MapPartitionsRDD[202] at textFile at <console>:33\ncount: Long = 541910\nsample3: Array[String] = Array(569457,22329,ROUND CONTAINER SET OF 5 RETROSPOT,1,10/4/2011 11:29,1.65,14606,United Kingdom, 571265,22530,MAGIC DRAWING SLATE DOLLY GIRL ,2,10/16/2011 11:31,0.42,16674,United Kingdom, 563893,90064B,BLACK VINTAGE  CRYSTAL EARRINGS,1,8/19/2011 17:10,3.75,16330,United Kingdom)\nlongRow: Long = 0\nres54: retailRDD.type = hdfs:///user/talha746/datasets/online_retail/online-retail-dataset_clean.txt MapPartitionsRDD[202] at textFile at <console>:33\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=95","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=96","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=97","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=98"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085689_812978461","id":"20190519-105016_1691323616","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:02:14+0000","dateFinished":"2020-03-18T02:02:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"text":"//Making some utilities and make your life easier :)\nval printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\nval printRdd3Samples = (rdd: org.apache.spark.rdd.RDD[_]) => printRddNSamples(rdd, 3)\nval printRddTopN = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.take(n).foreach(println)\nval bars = \"---------\"\nval printMsg = (msg:String) => println(bars+msg+bars)\n","user":"anonymous","dateUpdated":"2020-03-18T02:02:21+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"printRddNSamples: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\nprintRdd3Samples: org.apache.spark.rdd.RDD[_] => Unit = <function1>\nprintRddTopN: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\nbars: String = ---------\nprintMsg: String => Unit = <function1>\n"}]},"apps":[],"jobName":"paragraph_1584230085690_-1885487150","id":"20190519-192640_1954412488","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:02:21+0000","dateFinished":"2020-03-18T02:02:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"text":"%md\n### Spark RDD Transormations and Actions\n\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a>\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a>\n- Databrick RDD operations http://bit.ly/30ez9IG\n- Spark: The Definitive Guide Chapter 12 (required) & Chapter 13 (Optional)\n\n#### RDD Actions\n1. Get the first element from `retailRDD`\n2. Get the first 5 elements from `retailRDD` as an array.\n3. Get all elements from `retailRDD` as an array\n4. Get random 5 elements from `retailRDD` as an array\n5. Save all elements from `retailRDD` to local file `hdfs:///tmp/text.txt`\n\nSample outputs:\n```bash\n#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n```\n\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark RDD Transormations and Actions</h3>\n<ul>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a></li>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a></li>\n  <li>Databrick RDD operations <a href=\"http://bit.ly/30ez9IG\">http://bit.ly/30ez9IG</a></li>\n  <li>Spark: The Definitive Guide Chapter 12 (required) &amp; Chapter 13 (Optional)</li>\n</ul>\n<h4>RDD Actions</h4>\n<ol>\n  <li>Get the first element from <code>retailRDD</code></li>\n  <li>Get the first 5 elements from <code>retailRDD</code> as an array.</li>\n  <li>Get all elements from <code>retailRDD</code> as an array</li>\n  <li>Get random 5 elements from <code>retailRDD</code> as an array</li>\n  <li>Save all elements from <code>retailRDD</code> to local file <code>hdfs:///tmp/text.txt</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085690_699842248","id":"20190519-115905_2023471169","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"text":"//First lets filter out the first row again on retailRDD\nval filteredRDD = retailRDD.filter(y => y.split(\",\")(0) != \"InvoiceNo\")\n\n//RDD Action 1.\nval Action1 = filteredRDD.first\n\n//RDD Action 2.\nval Action2 = filteredRDD.take(5)\n\n//RDD Action 3.\nval Action3 = filteredRDD.collect.length","user":"anonymous","dateUpdated":"2020-03-18T02:03:49+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":297.2,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filteredRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[206] at filter at <console>:31\nAction1: String = 536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\nAction2: Array[String] = Array(536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom, 536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom, 536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom)\nAction3: Int = 541909\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=105","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=106","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=107"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085690_-1588894822","id":"20190519-122034_629713430","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:03:49+0000","dateFinished":"2020-03-18T02:03:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"text":"//RDD Action 4.\nval Action4 = filteredRDD.takeSample(false, 5)\n\n//RDD Action 5.\nretailRDD.saveAsTextFile(\"hdfs:///tmp/text3.txt\")\n//This saves retailRDD to the specified location as a text file","user":"anonymous","dateUpdated":"2020-03-18T02:04:07+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Action4: Array[String] = Array(581579,23570,TRADITIONAL PICK UP STICKS GAME ,12,12/9/2011 12:19,1.25,17581,United Kingdom, 575753,23312,VINTAGE CHRISTMAS GIFT SACK,1,11/11/2011 10:41,4.15,17841,United Kingdom, 579297,22485,SET OF 2 WOODEN MARKET CRATES,2,11/29/2011 11:23,24.96,,United Kingdom, 549564,22115,METAL SIGN EMPIRE TEA,3,4/10/2011 14:16,2.95,18129,United Kingdom, 541104,85203,HANGING WOOD AND FELT BUTTERFLY ,12,1/13/2011 14:29,0.42,,United Kingdom)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=108","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=109","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=110"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584484678118_2093374636","id":"20200317-223758_855416089","dateCreated":"2020-03-17T22:37:58+0000","dateStarted":"2020-03-18T02:04:07+0000","dateFinished":"2020-03-18T02:04:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584497112603_-1250474385","id":"20200318-020512_1774723697","dateCreated":"2020-03-18T02:05:12+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"text":"%md\n#### RDD Transformations\nRDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. `printRddNSamples` uses `takeSample` action) \n\n1. Get all sales from \"United Kingdom\" (hint: use fileter)\n2. Compare `sample` and `takeSample`\n\nSampel outputs:\n```bash\n#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n```","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD Transformations</h4>\n<p>RDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. <code>printRddNSamples</code> uses <code>takeSample</code> action) </p>\n<ol>\n  <li>Get all sales from &ldquo;United Kingdom&rdquo; (hint: use fileter)</li>\n  <li>Compare <code>sample</code> and <code>takeSample</code></li>\n</ol>\n<p>Sampel outputs:</p>\n<pre><code class=\"bash\">#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085691_2038264863","id":"20190917-181850_863623231","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"text":"//RDD Transformations 1.\nval UKSalesCount = retailRDD.map(row => row.split(\",\")).filter(x => x(7) equals \n                \"United Kingdom\").count\n\n//RDD Transformations 2.\n//sample returns RDD of Strings so lets take its count\nval sampleExample = retailRDD.sample(false, 0.5)\nval exampleCount = sampleExample.count\n\n//takeSample returns radom elements as an array\nval takeSampleExample = retailRDD.takeSample(false, 2)\n","user":"anonymous","dateUpdated":"2020-03-18T02:06:48+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"UKSalesCount: Long = 495478\nsampleExample: org.apache.spark.rdd.RDD[String] = PartitionwiseSampledRDD[219] at sample at <console>:36\nexampleCount: Long = 271406\ntakeSampleExample: Array[String] = Array(541869,21773,DECORATIVE ROSE BATHROOM BOTTLE,1,1/24/2011 9:35,2.46,,United Kingdom, 537472,84347,ROTATING SILVER ANGELS T-LIGHT HLDR,6,12/7/2010 11:11,2.55,12971,United Kingdom)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=119","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=120","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=121","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=122"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085691_1870120889","id":"20190519-124053_1683164197","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:06:48+0000","dateFinished":"2020-03-18T02:06:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:180"},{"text":"%md\n### Pair RDD (KeyValue)\nSo far, each element in `retailRDD` is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let's construct Pair RDDs from `retailRDD` in order to perform more advanced computations.\n\n**RDD vs PairRDD**:\n\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pair RDD (KeyValue)</h3>\n<p>So far, each element in <code>retailRDD</code> is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let&rsquo;s construct Pair RDDs from <code>retailRDD</code> in order to perform more advanced computations.</p>\n<p><strong>RDD vs PairRDD</strong>:</p>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085691_1661798933","id":"20190519-125017_38292448","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:181"},{"text":"%md\n\n#### Questions 1.0\n\nTrnasform each element in `retailRDD` to a key value pair (as a tuple) as following\n\n```\nkey = country\nvalue = amount (e.g. Quantity * UnitPrice)\n```\n\nhint: \n\n- use `rdd.map`\n- Use `row.split(\",\")` to tokenize the row\n- Cast quanitity to int while parsing the row\n- Cast price to double\n\n**Sample output**:\n\n```\n//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n```\n\n","user":"anonymous","dateUpdated":"2020-03-18T02:07:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.0</h4>\n<p>Trnasform each element in <code>retailRDD</code> to a key value pair (as a tuple) as following</p>\n<pre><code>key = country\nvalue = amount (e.g. Quantity * UnitPrice)\n</code></pre>\n<p>hint: </p>\n<ul>\n  <li>use <code>rdd.map</code></li>\n  <li>Use <code>row.split(&quot;,&quot;)</code> to tokenize the row</li>\n  <li>Cast quanitity to int while parsing the row</li>\n  <li>Cast price to double</li>\n</ul>\n<p><strong>Sample output</strong>:</p>\n<pre><code>//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085691_1586072973","id":"20190519-195132_1947538683","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:07:40+0000","dateFinished":"2020-03-18T02:07:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:182"},{"text":"%spark\n//Qustion 1.0 soutuion\n\nval keyValuePair = (row : String) => {\n    val eachColumn = row.split(\",\")\n    val quantity = eachColumn(3)\n    val unitPrice = eachColumn(5)\n    val country = eachColumn(7)\n    val value = quantity.toInt * unitPrice.toDouble\n    (country, value)\n}\n\n//To get this to work i needed to filter out the first line\n// that has the column header\nval filteredRDD = retailRDD.filter(y => y.split(\",\")(0) != \"InvoiceNo\")\n\nval countryRDD = filteredRDD.map(keyValuePair)\ncountryRDD.takeSample(false, 3)","user":"anonymous","dateUpdated":"2020-03-18T02:10:00+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":822,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"keyValuePair: String => (String, Double) = <function1>\nfilteredRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[224] at filter at <console>:45\ncountryRDD: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[225] at map at <console>:47\nres57: Array[(String, Double)] = Array((EIRE,15.0), (United Kingdom,33.0), (United Kingdom,7.5))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=125","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=126"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085692_-12588289","id":"20190519-125921_348001552","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:10:00+0000","dateFinished":"2020-03-18T02:10:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:183"},{"user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085692_823365538","id":"20190922-215540_2030793994","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:184"},{"text":"%md\n\n#### Questions 1.1\n\nCalculate total sales amount for each country and sort in descending order\n\n```sql\nSELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n```\n\n**Sample output**\n\n```bash\n//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n```\n\nHints:\n\n- implement `group by` with `rdd.reduceByKey` [doc](http://bit.ly/30fJHHs)\n- implement `order by` with `rdd.sortBy`","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.1</h4>\n<p>Calculate total sales amount for each country and sort in descending order</p>\n<pre><code class=\"sql\">SELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n</code></pre>\n<p>Hints:</p>\n<ul>\n  <li>implement <code>group by</code> with <code>rdd.reduceByKey</code> <a href=\"http://bit.ly/30fJHHs\">doc</a></li>\n  <li>implement <code>order by</code> with <code>rdd.sortBy</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085692_1861977231","id":"20190519-195238_1235609517","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"text":"%spark\n//Question 1.1\n//I used rdd.reduceByKey and rdd.sortBy to do this query\nval salesByCtryRDD = countryRDD.reduceByKey((x : Double, y : Double) => x + y).sortBy(_._2, ascending = false)\n//take will return the first 3 elements in sorted order\nsalesByCtryRDD.take(3)","user":"anonymous","dateUpdated":"2020-03-18T02:11:00+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"salesByCtryRDD: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[232] at sortBy at <console>:34\nres58: Array[(String, Double)] = Array((United Kingdom,8187806.363998696), (Netherlands,284661.539999999), (EIRE,263276.81999999884))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=127","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=128"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085693_-2058342336","id":"20190519-195236_1582695577","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:11:01+0000","dateFinished":"2020-03-18T02:11:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085693_-73645581","id":"20190922-215553_1555963294","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"text":"%md\n\n#### Questions 2.0\n\nLet's assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)\n\n```sql\nSELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n```\n\n**Sample output**\n\n```\n//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n```\n\n**hints:**\n\n- Generate a new KV pair RDD `(country, id)`\n- use `rdd.reduceByKey` find the smallest\n- ID must be a numric number\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 2.0</h4>\n<p>Let&rsquo;s assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)</p>\n<pre><code class=\"sql\">SELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code>//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n</code></pre>\n<p><strong>hints:</strong></p>\n<ul>\n  <li>Generate a new KV pair RDD <code>(country, id)</code></li>\n  <li>use <code>rdd.reduceByKey</code> find the smallest</li>\n  <li>ID must be a numric number</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085693_2128994552","id":"20190519-195157_1405617071","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"text":"%spark\n//Question 2.0\n//First lets filter out the first row again on retailRDD\nval filteredRDD = retailRDD.filter(y => y.split(\",\")(0) != \"InvoiceNo\")\n\n//Next week need to take out lines without customerId\n//and return RDD of (country, customerId)\nval resultRDD = filteredRDD.filter(y => y.split(\",\")(6) != \"\").map(x => {\n    val columns = x.split(\",\")\n    val customerId = columns(6).toInt\n    val country = columns(7)\n    (country, customerId)\n})\n\n//Now we need to get the min(cusotmerId) for each country and I do this with reduceByKey\nval result2min = resultRDD.reduceByKey((x: Int, y: Int) => if (x < y) x else y)\nval result2 = result2min.take(3)\n\n","user":"anonymous","dateUpdated":"2020-03-18T02:12:23+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filteredRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[233] at filter at <console>:34\nresultRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[235] at map at <console>:38\nresult2min: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[236] at reduceByKey at <console>:46\nresult2: Array[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=129"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085693_353579004","id":"20190519-144439_1578143376","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:12:23+0000","dateFinished":"2020-03-18T02:12:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085694_1380623445","id":"20190922-215609_1113478409","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"text":"%md\n\n### Question 2.1\n\nIt's inconvenient to tokenize each row in every operation. Instead, we count convert `retailRDD[String]` to `itemsRdd:RDD[Item]` where `Item` is a case class as following:\n\n`case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)`\n\nIn this way, you only parse each row only once here and you can reuse `itemsRdd` in the rest of the questions.\n\n**Sample outputs**\n```scala\n//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n```\n\n**Hints**:\n\n- write a function to convert a row to a Item, e.g. \n```\nval parseRow2Item: (String) => Item = (row: String) => {\n    //complete body\n}\n```\n- Covert all rows to items, e.g. `val itemsRdd =  retailRDD.map(parseRow2Item)`\n","user":"anonymous","dateUpdated":"2020-03-18T00:41:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Question 2.1</h3>\n<p>It&rsquo;s inconvenient to tokenize each row in every operation. Instead, we count convert <code>retailRDD[String]</code> to <code>itemsRdd:RDD[Item]</code> where <code>Item</code> is a case class as following:</p>\n<p><code>case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)</code></p>\n<p>In this way, you only parse each row only once here and you can reuse <code>itemsRdd</code> in the rest of the questions.</p>\n<p><strong>Sample outputs</strong></p>\n<pre><code class=\"scala\">//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n</code></pre>\n<p><strong>Hints</strong>:</p>\n<ul>\n  <li>\n  <p>write a function to convert a row to a Item, e.g. </p>\n  <pre><code>val parseRow2Item: (String) =&gt; Item = (row: String) =&gt; {\n//complete body\n}\n</code></pre></li>\n  <li>Covert all rows to items, e.g. <code>val itemsRdd =  retailRDD.map(parseRow2Item)</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085694_-434832445","id":"20190921-180308_1963749922","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T00:41:05+0000","dateFinished":"2020-03-18T00:41:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"text":"%spark\n//Question 2.1\n\ncase class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], country:String)\n\n//Some columns may be empty so we can set those to None\nval parseRow2Item : (String) => Item = (row : String) => {\n    val columns = row.split(\",\")\n    val invoiceNo = columns(0)\n    val stockCode = columns(1)\n    val description = if (columns(2).equals(\"\")) None else Some(columns(2))\n    val quantity = columns(3).toInt\n    val invoiceDate = columns(4)\n    val unitPrice = columns(5).toDouble\n    val customerID = if (columns(6).equals(\"\")) None else Some(columns(6).toInt)\n    val country = columns(7)\n    \n    //Now we just consturct Item with our extracted column values\n    Item(invoiceNo, stockCode, description, quantity, invoiceDate, unitPrice, customerID, country)\n}\n\n//First lets filter out the first row again on retailRDD\nval filteredRDD = retailRDD.filter(y => y.split(\",\")(0) != \"InvoiceNo\")\n\n// Create the Item RDD by mapping rows of the Retail RDD to Items\nval itemsRDD = filteredRDD.map(parseRow2Item)\nitemsRDD.takeSample(false, 3)\n\n\n","user":"anonymous","dateUpdated":"2020-03-18T02:15:25+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Item\nparseRow2Item: String => Item = <function1>\nfilteredRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[237] at filter at <console>:43\nitemsRDD: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[238] at map at <console>:46\nres59: Array[Item] = Array(Item(550124,84988,Some(SET OF 72 PINK HEART PAPER DOILIES),12,4/14/2011 12:41,1.45,Some(12853),United Kingdom), Item(549746,22969,Some(HOMEMADE JAM SCENTED CANDLES),96,4/12/2011 10:38,1.25,Some(15251),United Kingdom), Item(546984,21399,Some(BLUE POLKADOT COFFEE MUG),2,3/18/2011 12:35,0.39,Some(15019),United Kingdom))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=130","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=131"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085694_399942984","id":"20190921-180433_1776305327","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:15:25+0000","dateFinished":"2020-03-18T02:15:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085694_-1467063129","id":"20190922-215634_337314214","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"text":"%md\n### Questions 2.2\n\nRe-implement questions 1.1 & 2.1 using itemsRdd (`RDD[Item]`)","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Questions 2.2</h3>\n<p>Re-implement questions 1.1 &amp; 2.1 using itemsRdd (<code>RDD[Item]</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085695_-1968955050","id":"20190922-140917_1244358721","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"%spark\n//Question 2.2\n//For 1.1, I used rdd.reduceByKey and rdd.sortBy after mapping\n\nval salesByCtryRDD2 = itemsRDD.map(item => (item.country, item.quantity * item.unitPrice)).reduceByKey((x : Double, y : Double) => x + y).sortBy(_._2, ascending = false)\n\n//take will return the first 3 elements in sorted order\nsalesByCtryRDD2.take(3)","user":"anonymous","dateUpdated":"2020-03-18T02:16:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"salesByCtryRDD2: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[246] at sortBy at <console>:37\nres60: Array[(String, Double)] = Array((United Kingdom,8187806.363998696), (Netherlands,284661.539999999), (EIRE,263276.81999999884))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=132","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=133"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085695_468920274","id":"20190922-141025_1178356031","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:16:28+0000","dateFinished":"2020-03-18T02:16:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085695_-2114430704","id":"20190922-215642_1393968112","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"%md\n#### Questions 3\n\nFind number of customers.\n\n```\nSELECT distinct(customerId)\nFROM retail\n```\n\n**Sample output**\n```scala\n//resultRdd.count\nres458: Long = 4373\n```","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 3</h4>\n<p>Find number of customers.</p>\n<pre><code>SELECT distinct(customerId)\nFROM retail\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//resultRdd.count\nres458: Long = 4373\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085695_1778650267","id":"20190519-195407_556558321","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%spark\n//Question 3\n\nval customers = itemsRDD.filter(item => item.customerID.nonEmpty).map(item => item.customerID.get).distinct\n\ncustomers.count","user":"anonymous","dateUpdated":"2020-03-18T02:46:55+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"customers: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[251] at distinct at <console>:37\nres61: Long = 4372\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=134"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085696_641234348","id":"20190519-194831_1486531342","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:16:58+0000","dateFinished":"2020-03-18T02:16:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584230085696_-2054597199","id":"20190922-215651_1847295545","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%md\n#### Question 4\n\nFind out the number of invoices/purchases for each customer.\n> Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)\n\n```sql\nSELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n```\n\n**Sample output**\n```scala\n//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n```\n","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Question 4</h4>\n<p>Find out the number of invoices/purchases for each customer.</p>\n<blockquote>\n  <p>Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)</p>\n</blockquote>\n<pre><code class=\"sql\">SELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085704_1485579832","id":"20190519-195310_661372203","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"%spark\n//Question 4\n//First need to filter the non empty customerIDs, then can map to get number of purchases for each customer\nval filteredItems = itemsRDD.filter(item => item.customerID.nonEmpty)\n\nval invoicesPerCustomer = filteredItems.map(item => (item.customerID, item.invoiceNo)).aggregateByKey(0)((y : Int, x : String) => y + 1, (a : Int, b) => a + b)\n\ninvoicesPerCustomer.takeSample(false, 3, 3)\n\n","user":"anonymous","dateUpdated":"2020-03-18T02:29:05+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filteredItems: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[311] at filter at <console>:39\ninvoicesPerCustomer: org.apache.spark.rdd.RDD[(Option[Int], Int)] = ShuffledRDD[313] at aggregateByKey at <console>:41\nres80: Array[(Option[Int], Int)] = Array((Some(14451),109), (Some(15036),127), (Some(17004),44))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=164","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=165"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085705_1220028061","id":"20190922-190215_1478690977","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:29:05+0000","dateFinished":"2020-03-18T02:29:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"text":"%md\n#### SPARK UI\nEvery Spark Job has a web UI for monitroing and debuging purposes.\nGo to GCP > your hadoop cluster > web interfaces > Spark History Server > Spark UI. ","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>SPARK UI</h4>\n<p>Every Spark Job has a web UI for monitroing and debuging purposes.<br/>Go to GCP &gt; your hadoop cluster &gt; web interfaces &gt; Spark History Server &gt; Spark UI.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085705_760499477","id":"20190521-114127_1095254606","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%md\n#### RDD join\n\nPrint `customerId, name, country` using `customers.txt` and  `online-retail-dataset_clean.txt` files\n\n1. Load `datasets/online_retail/customers.txt` to `RDD[Customer]` where `Customer` is a case class as following\n`case class Customer(customerId:Int, name: String)`\n2. Join `RDD[Customer]` with `RDD[item]` on `customerId`\n3. Select uniq `customerId, name, country`\n\n```sql\nSELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n```\n\n**Sample Output**\n```bash\n//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n```","user":"anonymous","dateUpdated":"2020-03-14T23:54:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD join</h4>\n<p>Print <code>customerId, name, country</code> using <code>customers.txt</code> and <code>online-retail-dataset_clean.txt</code> files</p>\n<ol>\n  <li>Load <code>datasets/online_retail/customers.txt</code> to <code>RDD[Customer]</code> where <code>Customer</code> is a case class as following<br/><code>case class Customer(customerId:Int, name: String)</code></li>\n  <li>Join <code>RDD[Customer]</code> with <code>RDD[item]</code> on <code>customerId</code></li>\n  <li>Select uniq <code>customerId, name, country</code></li>\n</ol>\n<pre><code class=\"sql\">SELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n</code></pre>\n<p><strong>Sample Output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584230085705_-913332908","id":"20190519-183851_617743118","dateCreated":"2020-03-14T23:54:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"text":"%spark\n\ncase class Customer(customerId  : Int, name : String)\n\n//Question 1.\nval customersTxt = sc.textFile(\"hdfs:///user/talha746/datasets/online_retail/customers.txt\")\n\n//Now we can convert that RDD to customerRDD\nval customers = customersTxt.map (row => {\n    val tokens = row.split(\",\")\n    Customer(tokens(0).toInt, tokens(1))\n})\n\ncustomers.take(3)\n","user":"anonymous","dateUpdated":"2020-03-18T02:45:15+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Customer\ncustomersTxt: org.apache.spark.rdd.RDD[String] = hdfs:///user/talha746/datasets/online_retail/customers.txt MapPartitionsRDD[387] at textFile at <console>:21\ncustomers: org.apache.spark.rdd.RDD[Customer] = MapPartitionsRDD[388] at map at <console>:24\nres86: Array[Customer] = Array(Customer(15930, Philip V. Bradford), Customer(17796, Alvin V. Ellison), Customer(15550, Nero D. Walls))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=185"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584230085706_2078150763","id":"20190922-193513_1093352183","dateCreated":"2020-03-14T23:54:45+0000","dateStarted":"2020-03-18T02:45:15+0000","dateFinished":"2020-03-18T02:45:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%spark\n\n//Question 2.\n//Need to set key for both customers and itemsRDD to be customerID\n//We can use leftOuterJoin to join these two now because some customers may not have invoices\nval joinedRDD = customers.keyBy(_.customerId).leftOuterJoin(itemsRDD.filter(item => item.customerID.nonEmpty).keyBy(_.customerID.get))","user":"anonymous","dateUpdated":"2020-03-18T02:46:04+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"joinedRDD: org.apache.spark.rdd.RDD[(Int, (Customer, Option[Item]))] = MapPartitionsRDD[394] at leftOuterJoin at <console>:42\n"}]},"apps":[],"jobName":"paragraph_1584499195843_1902752851","id":"20200318-023955_1034534385","dateCreated":"2020-03-18T02:39:55+0000","dateStarted":"2020-03-18T02:46:04+0000","dateFinished":"2020-03-18T02:46:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:205"}],"name":"Jarvis/1-SparkRDD_pub","id":"2F3KXDU55","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[],"hive:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}