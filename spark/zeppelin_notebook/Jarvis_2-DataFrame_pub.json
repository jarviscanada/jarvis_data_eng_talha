{"paragraphs":[{"text":"%md\n# Creating Dataframes\n\nSpark DF allows you to create dataframes from the following sources.\n\n- Option 1: Spark Sequence (built-in data structure)\n- Option 2: External sources\n    - Hive tables (connect to Hive metastore)\n    - Structured and semi-structured files from different file systems (e.g., HDFS, GS, S3, local FS)","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating Dataframes</h1>\n<p>Spark DF allows you to create dataframes from the following sources.</p>\n<ul>\n  <li>Option 1: Spark Sequence (built-in data structure)</li>\n  <li>Option 2: External sources\n    <ul>\n      <li>Hive tables (connect to Hive metastore)</li>\n      <li>Structured and semi-structured files from different file systems (e.g., HDFS, GS, S3, local FS)</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462579_1448519165","id":"20200119-073223_398987810","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54"},{"text":"%md\n## Option 1: Spark Sequence\n\nSpark DF can convert a sequence of tuples to Spark DF.\ne.g., `Seq[(String, Double, String, String)]`\n\n- A tuple corresponds to a DF row.\n- An element in a tuple corresponds to a column to a particular row.\n\nPlease run and learn the paragraph below. Feel free to modify the code to test your queries.","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Option 1: Spark Sequence</h2>\n<p>Spark DF can convert a sequence of tuples to Spark DF.<br/>e.g., <code>Seq[(String, Double, String, String)]</code></p>\n<ul>\n  <li>A tuple corresponds to a DF row.</li>\n  <li>An element in a tuple corresponds to a column to a particular row.</li>\n</ul>\n<p>Please run and learn the paragraph below. Feel free to modify the code to test your queries.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462592_-1678527269","id":"20190519-201210_1157722001","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"%md\n### Scala Implicit Conversions (optional)\n\nIn short, you have to `import spark.implicits._` to convert/cast a `Seq[(String, Double, String, String)]` to a Spark `DataFrame`. (e.g. `lineTupleSeq.toDF`)\n\nThis is called implicit conversions in Scala. In this case, `spark.implicits.localSeqToDatasetHolder` creates a Dataset from a local Seq.\n\nSpark Scala Docs:\n\n- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a>\n- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a>","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Scala Implicit Conversions (optional)</h3>\n<p>In short, you have to <code>import spark.implicits._</code> to convert/cast a <code>Seq[(String, Double, String, String)]</code> to a Spark <code>DataFrame</code>. (e.g. <code>lineTupleSeq.toDF</code>)</p>\n<p>This is called implicit conversions in Scala. In this case, <code>spark.implicits.localSeqToDatasetHolder</code> creates a Dataset from a local Seq.</p>\n<p>Spark Scala Docs:</p>\n<ul>\n  <li>\n  <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a></li>\n  <li>\n  <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462592_757438895","id":"20190520-102917_1809142825","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"/**\n * Each line/record/row must be a Tuple\n * e.g.  Tuple(AAPL,110.5,2018-02-01,Apple)\n * \n * Lines are grouped into a Seq\n * List(\n *   (AAPL,110.5,2018-02-01,Apple),\n *   (AMZN,1500.52,2018-02-01,Ammazon.com),\n *   (FB,170.01,2018-02-01,Facebook)\n * )\n */\nval lineTuple1 = (\"AAPL\",110.5,\"2018-02-01\",\"Apple\")\nval lineTuple2 = (\"AMZN\",1500.52,\"2018-02-01\",\"Ammazon.com\")\nval lineTuple3 = (\"FB\",170.01,\"2018-02-01\",\"Facebook\")\nval lineTupleSeq = Seq(lineTuple1,lineTuple2,lineTuple3)\n\n//To use toDF, you must import this (see next section for details)\n//In fact Zeppellin interpreter already imported this for you\nimport spark.implicits._\nval stockDf = lineTupleSeq.toDF(\"ticker\",\"price\", \"date\", \"companyName\")\nprintln(\">>Print Schema\")\nstockDf.printSchema\n\n//SELECT * FROM stock LIMIT 3\nprintln(\">>Print 3 rows\")\nstockDf.show(3)\n\n//SELECT companyName AS company_name, price FROM stock\nprintln(\">>Print all rows with renamed columns\")\nstockDf.select(col(\"companyName\").as(\"company_name\"), col(\"price\")).show()\n\n//Use SQL to query dataframe\n//creating a tmep view\nstockDf.createOrReplaceTempView(\"stock\")\n//execute query\nval filteredDf = spark.sql(\"SELECT * FROM stock WHERE price > 100.0\")\nprintln(\">>SQL\")\nfilteredDf.show()\n\n","user":"anonymous","dateUpdated":"2020-03-18T04:50:25+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":">>Print Schema\nroot\n |-- ticker: string (nullable = true)\n |-- price: double (nullable = false)\n |-- date: string (nullable = true)\n |-- companyName: string (nullable = true)\n\n>>Print 3 rows\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01|Ammazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\n>>Print all rows with renamed columns\n+------------+-------+\n|company_name|  price|\n+------------+-------+\n|       Apple|  110.5|\n| Ammazon.com|1500.52|\n|    Facebook| 170.01|\n+------------+-------+\n\n>>SQL\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01|Ammazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\nlineTuple1: (String, Double, String, String) = (AAPL,110.5,2018-02-01,Apple)\nlineTuple2: (String, Double, String, String) = (AMZN,1500.52,2018-02-01,Ammazon.com)\nlineTuple3: (String, Double, String, String) = (FB,170.01,2018-02-01,Facebook)\nlineTupleSeq: Seq[(String, Double, String, String)] = List((AAPL,110.5,2018-02-01,Apple), (AMZN,1500.52,2018-02-01,Ammazon.com), (FB,170.01,2018-02-01,Facebook))\nimport spark.implicits._\nstockDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\nfilteredDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=229","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=230"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462593_-1949095091","id":"20190519-201416_412351679","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:50:25+0000","dateFinished":"2020-03-18T04:50:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"%md\n## Option 2: External sources\nThe most common way to create Spark DFs is to read data/files from external sources. Spark has built-in features to parse CSV, JSON, and many other semistructured and structured files.\n\nPlease run and learn the paragraph below. Feel free to modify the code to test your queries.","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Option 2: External sources</h2>\n<p>The most common way to create Spark DFs is to read data/files from external sources. Spark has built-in features to parse CSV, JSON, and many other semistructured and structured files.</p>\n<p>Please run and learn the paragraph below. Feel free to modify the code to test your queries.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462600_-1719169470","id":"20190520-104920_1833330750","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"text":"//Read CSV file to df\n//local or hdfs path\nval path = \"hdfs:///user/talha746/datasets/online_retail/online-retail-dataset.txt\"\n\n//spark.read is able to handle csv formats\nval retailDf = spark.read.format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(path)\n\nretailDf.printSchema\nretailDf.show(3)\nretailDf.show(3,false)","user":"anonymous","dateUpdated":"2020-03-18T04:50:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\npath: String = hdfs:///user/talha746/datasets/online_retail/online-retail-dataset.txt\nretailDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=231","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=232","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=233","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=234"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462600_-80940182","id":"20190520-095229_630927102","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:50:32+0000","dateFinished":"2020-03-18T04:50:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"text":"%md\n### Column Type Cast\nIn the previous paragraph, the data type of the `InvoiceDate` column is String instead of `timestamp`. In this practice, you need to cast `InvoiceDate` column to Spark `timestamp` data type.\n\n```bash\nresultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n```\n","user":"anonymous","dateUpdated":"2020-03-18T03:10:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Column Type Cast</h3>\n<p>In the previous paragraph, the data type of the <code>InvoiceDate</code> column is String instead of <code>timestamp</code>. In this practice, you need to cast <code>InvoiceDate</code> column to Spark <code>timestamp</code> data type.</p>\n<pre><code class=\"bash\">resultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462601_357459522","id":"20190520-085947_2007764287","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T03:10:05+0000","dateFinished":"2020-03-18T03:10:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"//write code to cast InvoiceData from String to timestamp\n//save the result DF as `val restaulCastDf`\n//please see sample result at the end of this paragraph (below the editor)\nval retailCastDf = retailDf.withColumn(\"InvoiceDate\", to_timestamp($\"InvoiceDate\", \"MM/dd/yyyy HH:mm\"))\n\n\n//print schema\nretailCastDf.printSchema\n//print rows to verify\nretailCastDf.show(3)\n//Cache DF in memory since it will be accessed frequently\nretailCastDf.cache","user":"anonymous","dateUpdated":"2020-03-18T04:50:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\nretailCastDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nres99: retailCastDf.type = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=235"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462601_-1896116683","id":"20190519-215300_721200493","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:50:56+0000","dateFinished":"2020-03-18T04:50:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%md\n# DF Operations","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DF Operations</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462602_1046607751","id":"20191010-103454_2098588366","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%md\n## DataFrame SELECT\nImplement the following SQL queries using dataframe. Compare different select syntax.\n\n```sql\nSELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n```","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame SELECT</h2>\n<p>Implement the following SQL queries using dataframe. Compare different select syntax.</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462602_1789993536","id":"20190519-221054_1925024171","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"//SELECT * from retail limit 1;\nretailCastDf.show(3)\n\nimport org.apache.spark.sql.functions._\n\n//select InvoiceNo,CustomerID,Country from retail limit 1;\nretailCastDf.select(\"InvoiceNo\").show(1)\n\n//Different ways of select \nretailCastDf.select($\"InvoiceNo\").show(1)\nretailCastDf.select('InvoiceNo).show(1)\nretailCastDf.select(col(\"InvoiceNo\")).show(1)\nretailCastDf.select(retailCastDf.col(\"InvoiceNo\")).show(1)\nretailCastDf.select(expr(\"InvoiceNo\")).show(1)\n\n//ERROR: cannot mix \n//retailCastDf.select($\"InvoiceNo\", \"StockCode\").show(1)\n\n//expr or selectExpr is most powerful and close to SQL syntax\n\n//SELECT InvoiceNo as invoiceId from retail limit 1;\n//col(\"InvoiceNo\").as(\"invoiceId\")\nretailCastDf.select(expr(\"InvoiceNo as invoiceId\")).show(1)\nretailCastDf.selectExpr(\"InvoiceNo as invoiceId\").show(1)\n\n//SELECT * from retail limit 1;\nretailCastDf.selectExpr(\"*\").show(1)\n\n//select max(UnitPrice) as maxUnitPrice from retail\nretailCastDf.selectExpr(\"max(UnitPrice) as maxUnitPrice\").show \n","user":"anonymous","dateUpdated":"2020-03-18T04:51:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 1 row\n\norg.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'default' not found;\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireDbExists(SessionCatalog.scala:174)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.functionExists(SessionCatalog.scala:1076)\n  at org.apache.spark.sql.hive.HiveSessionCatalog.functionExists(HiveSessionCatalog.scala:175)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1212)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1211)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:85)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:85)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:76)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:138)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:137)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:137)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1211)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1210)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3307)\n  at org.apache.spark.sql.Dataset.select(Dataset.scala:1318)\n  at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1353)\n  ... 55 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=236","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=237","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=238","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=239","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=240","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=241","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=242","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=243","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=244","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=245"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462602_201030362","id":"20190519-211701_1956303781","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:51:12+0000","dateFinished":"2020-03-18T04:51:14+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%md\n## DataFrame filtering (WHERE)\n\nImplement the following SQL query\n\n```sql\nSELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n```\n\nSample results\n```\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n```","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame filtering (WHERE)</h2>\n<p>Implement the following SQL query</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n</code></pre>\n<p>Sample results</p>\n<pre><code>+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462603_-636589791","id":"20190519-221114_648626738","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"//write your code here and print result\n\nretailCastDf.where('InvoiceNo === 536365).limit(2).show\nretailCastDf.where('InvoiceNo === 536365).show(2)\n","user":"anonymous","dateUpdated":"2020-03-18T04:51:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=246","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=247"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462603_1097469362","id":"20190519-201625_2028882244","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:51:23+0000","dateFinished":"2020-03-18T04:51:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584481462604_316178856","id":"20191007-145852_244125478","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"%md\n\n# DF Exercises\nIn the following phrargraphs, you will be asked to solve some bussiness question using Spark Dataframes. However, you can use Spark SQL to verify you solution. ","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DF Exercises</h1>\n<p>In the following phrargraphs, you will be asked to solve some bussiness question using Spark Dataframes. However, you can use Spark SQL to verify you solution.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462604_930350126","id":"20190520-123428_698724288","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"//Spark SQL exercies\n//register retailCastDf as `retail` view\nretailCastDf.createOrReplaceTempView(\"retail\")\n\n//execute SQL\nspark.sql(\"SELECT * FROM retail limit 10\").show","user":"anonymous","dateUpdated":"2020-03-18T04:53:22+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|     17850|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|     17850|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|     17850|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|     13047|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=249"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462604_1196697311","id":"20190520-142038_1683726413","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:53:22+0000","dateFinished":"2020-03-18T04:53:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md\n#### Q1: Find the top N largest invoices by the amount (`Quantity * UnitPrice`)\n\nNote: `InvoiceNo` will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)\n\n**Sample output**\n```bash\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n```","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q1: Find the top N largest invoices by the amount (<code>Quantity * UnitPrice</code>)</h4>\n<p>Note: <code>InvoiceNo</code> will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)</p>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462605_-321619810","id":"20190520-133812_405266917","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"text":"//write you DF solution here\n//please verify using SparkSQL\nval invoicesSorted = retailCastDf.groupBy($\"InvoiceNo\").agg(sum('Quantity * 'UnitPrice).as(\"Amount\")).orderBy('Amount.desc)\n\ninvoicesSorted.show(5)\n\n//verifying with SparkSQL\nspark.sql(\"select InvoiceNo, sum(Quantity * UnitPrice) as Amount from retail group by InvoiceNo order by Amount desc\").show(5)","user":"anonymous","dateUpdated":"2020-03-18T04:58:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\ninvoicesSorted: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=256","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=257"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462605_-1051965061","id":"20190519-215312_1016690251","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T04:58:40+0000","dateFinished":"2020-03-18T04:58:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584481462606_-1304273119","id":"20191007-145909_914572499","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"text":"%md\n#### Q2: Find the top N largest invoices by the amount and show receipt details\n\n```\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n```","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q2: Find the top N largest invoices by the amount and show receipt details</h4>\n<pre><code>+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462606_-1140616757","id":"20190520-124355_215736883","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"text":"//write you DF solution here\n//First we need to drop duplicates from retailCastDf\nval invoices = retailCastDf.dropDuplicates(\"InvoiceNo\")\n\n//Now we can join it with invoicesSorted table and then only output the columns we need\nval invoicesExtended = invoicesSorted.join(invoices, invoicesSorted(\"InvoiceNo\") === invoices(\"InvoiceNo\")).select(invoicesSorted(\"InvoiceNo\"), 'Amount, 'InvoiceDate, 'CustomerID, 'Country).orderBy('Amount.desc)\n\ninvoicesExtended.show(5)\n","user":"anonymous","dateUpdated":"2020-03-18T05:08:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\nonly showing top 5 rows\n\ninvoices: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 6 more fields]\ninvoicesExtended: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double ... 3 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=267","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=268"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462606_694239554","id":"20190520-122626_1736024345","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T05:07:12+0000","dateFinished":"2020-03-18T05:07:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"%md\n#### Q3: For each country, find the top N largest invoices by the amount and show receipt details\n\nUse `Window functions` and `rank()` function\n\nReadings:\n- https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n- https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\n- `Spark The Definitive Guide - page 134 - Windows Function`\n\n```\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n```\n<br>\n<br>\n**Hints**:\n- At high level, you need to create a new column which indicates amount rank by country\n  - Use `Windows` function which partition by (\"Country\") and order by amount\n  - User `Rank()` function create a new `rank` column for each row\n  - filter out rows where `rank > 2`","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q3: For each country, find the top N largest invoices by the amount and show receipt details</h4>\n<p>Use <code>Window functions</code> and <code>rank()</code> function</p>\n<p>Readings:<br/>- <a href=\"https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br/>- <a href=\"https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\">https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a><br/>- <code>Spark The Definitive Guide - page 134 - Windows Function</code></p>\n<pre><code>+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n</code></pre>\n<br>\n<br>\n<p><strong>Hints</strong>:<br/>- At high level, you need to create a new column which indicates amount rank by country<br/> - Use <code>Windows</code> function which partition by (&ldquo;Country&rdquo;) and order by amount<br/> - User <code>Rank()</code> function create a new <code>rank</code> column for each row<br/> - filter out rows where <code>rank &gt; 2</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462607_1613766454","id":"20190520-150543_915955507","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"text":"//write you DF solution here\n\n//Create a new rank column for each row using rank() function, we partition by country and order by the amound\nval topInvoices = invoicesExtended.withColumn(\"rank\", expr(\"rank() over (partition by Country order by Amount desc)\"))\n\n//Now we can just filter out rows where rank > 2\ntopInvoices.select('InvoiceNo, 'amount, 'InvoiceDate, 'CustomerID, 'Country).where('rank <= 2).show(10)","user":"anonymous","dateUpdated":"2020-03-18T05:17:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\nonly showing top 10 rows\n\ntopInvoices: org.apache.spark.sql.DataFrame = [InvoiceNo: string, Amount: double ... 4 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=287","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=288","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=289","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=290","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=291","http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=292"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462616_2073263094","id":"20190520-125029_1350468290","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T05:17:50+0000","dateFinished":"2020-03-18T05:18:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"text":"%md\n\n#### Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.\n\n\n```bash\ndailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n```\n\n```bash\nweeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n```\n\nReadings\n- https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/","user":"anonymous","dateUpdated":"2020-03-17T21:44:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.</h4>\n<pre><code class=\"bash\">dailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n</code></pre>\n<pre><code class=\"bash\">weeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n</code></pre>\n<p>Readings<br/>- <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584481462616_-403372049","id":"20190520-140931_1510736707","dateCreated":"2020-03-17T21:44:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"text":"//Daily\n//write you DF solution here\n\n//So first we need to group by setting a 1 day window \n//Then need to get the sum of that day and output the start day and sum\nval dailySum = retailCastDf.groupBy(window('InvoiceDate, \"1 day\").getField(\"start\").cast(\"date\").as(\"start\")).agg(sum('Quantity * 'UnitPrice).as(\"sum(amount)\")).orderBy('start)\n\n//Lets show the top 5 results\ndailySum.show(5)\n\n//Now we need to create a view in order to pot the diagram\ndailySum.createOrReplaceTempView(\"dailySales\")\n","user":"anonymous","dateUpdated":"2020-03-18T05:52:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------------+\n|     start|       sum(amount)|\n+----------+------------------+\n|2010-12-01| 58635.56000000026|\n|2010-12-02| 46207.27999999991|\n|2010-12-03|  45620.4599999999|\n|2010-12-05| 31383.95000000016|\n|2010-12-06|53860.180000000015|\n+----------+------------------+\nonly showing top 5 rows\n\ndailySum: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: date, sum(amount): double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=307"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462617_702968575","id":"20190520-181045_1661878813","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T05:41:55+0000","dateFinished":"2020-03-18T05:41:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"text":"%sql\n-- Plot daily diagram here\nselect start, `sum(amount)` from dailySales","user":"anonymous","dateUpdated":"2020-03-18T05:52:01+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"scatterChart","height":326,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"start":"string","sum(amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"pieChart":{}},"commonSetting":{},"keys":[],"groups":[],"values":[]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"start\tsum(amount)\n2010-12-01\t58635.56000000026\n2010-12-02\t46207.27999999991\n2010-12-03\t45620.4599999999\n2010-12-05\t31383.95000000016\n2010-12-06\t53860.180000000015\n2010-12-07\t45059.05000000015\n2010-12-08\t44189.839999999866\n2010-12-09\t52532.13000000003\n2010-12-10\t57404.91000000017\n2010-12-12\t17240.92000000005\n2010-12-13\t35379.34000000009\n2010-12-14\t42843.290000000205\n2010-12-15\t29443.69000000013\n2010-12-16\t48334.34999999993\n2010-12-17\t43534.19000000001\n2010-12-19\t7517.309999999992\n2010-12-20\t24741.750000000015\n2010-12-21\t47097.939999999915\n2010-12-22\t6134.569999999999\n2010-12-23\t11796.310000000025\n2011-01-04\t14950.480000000014\n2011-01-05\t-1566.2299999999464\n2011-01-06\t37392.73999999999\n2011-01-07\t27233.140000000065\n2011-01-09\t15710.800000000025\n2011-01-10\t24191.64\n2011-01-11\t67817.1300000002\n2011-01-12\t23958.77999999998\n2011-01-13\t20533.540000000005\n2011-01-14\t47377.2600000001\n2011-01-16\t7116.609999999981\n2011-01-17\t29256.000000000295\n2011-01-18\t18680.800000000003\n2011-01-19\t25585.81000000013\n2011-01-20\t17995.909999999974\n2011-01-21\t31978.440000000195\n2011-01-23\t10285.94999999997\n2011-01-24\t25555.620000000134\n2011-01-25\t27971.520000000113\n2011-01-26\t19493.320000000065\n2011-01-27\t21092.140000000076\n2011-01-28\t18567.76999999999\n2011-01-30\t6456.439999999991\n2011-01-31\t22364.650000000103\n2011-02-01\t28433.220000000023\n2011-02-02\t21048.45000000002\n2011-02-03\t23344.580000000096\n2011-02-04\t24994.170000000042\n2011-02-06\t3457.1099999999965\n2011-02-07\t25525.989999999983\n2011-02-08\t20728.140000000043\n2011-02-09\t16692.579999999976\n2011-02-10\t13427.539999999994\n2011-02-11\t20387.279999999984\n2011-02-13\t5535.399999999994\n2011-02-14\t26222.03000000017\n2011-02-15\t36842.58000000006\n2011-02-16\t24730.81000000009\n2011-02-17\t26361.87000000019\n2011-02-18\t15928.399999999987\n2011-02-20\t9578.890000000032\n2011-02-21\t23807.830000000176\n2011-02-22\t32292.62000000012\n2011-02-23\t26792.76000000015\n2011-02-24\t22655.830000000096\n2011-02-25\t18029.840000000007\n2011-02-27\t9491.049999999981\n2011-02-28\t21753.680000000186\n2011-03-01\t25471.71000000005\n2011-03-02\t18296.45\n2011-03-03\t35842.62000000009\n2011-03-04\t19474.8700000001\n2011-03-06\t9596.229999999987\n2011-03-07\t30525.58000000021\n2011-03-08\t25017.47000000018\n2011-03-09\t21907.120000000094\n2011-03-10\t25597.890000000112\n2011-03-11\t21995.28000000005\n2011-03-13\t4137.619999999995\n2011-03-14\t25864.590000000062\n2011-03-15\t20660.029999999955\n2011-03-16\t21182.640000000065\n2011-03-17\t38804.25000000005\n2011-03-18\t16770.459999999995\n2011-03-20\t21980.640000000098\n2011-03-21\t16370.270000000037\n2011-03-22\t31312.350000000228\n2011-03-23\t24029.070000000094\n2011-03-24\t36562.1000000001\n2011-03-25\t30656.03000000007\n2011-03-27\t8979.979999999996\n2011-03-28\t19207.030000000002\n2011-03-29\t70531.46999999978\n2011-03-30\t31489.25000000014\n2011-03-31\t31004.08000000019\n2011-04-01\t24391.780000000057\n2011-04-03\t6878.0999999999985\n2011-04-04\t25073.020000000182\n2011-04-05\t28353.83000000014\n2011-04-06\t17279.350000000064\n2011-04-07\t18229.00000000011\n2011-04-08\t23299.140000000112\n2011-04-10\t9363.879999999972\n2011-04-11\t22110.310000000063\n2011-04-12\t25124.250000000084\n2011-04-13\t23898.200000000023\n2011-04-14\t35295.580000000096\n2011-04-15\t28327.130999999976\n2011-04-17\t12704.299999999994\n2011-04-18\t32185.610000000288\n2011-04-19\t23837.650000000183\n2011-04-20\t28239.390000000025\n2011-04-21\t31198.60000000008\n2011-04-26\t30585.540000000157\n2011-04-27\t25590.56000000002\n2011-04-28\t21241.900000000063\n2011-05-01\t6964.659999999997\n2011-05-03\t19617.860000000135\n2011-05-04\t27462.3000000001\n2011-05-05\t28750.650000000238\n2011-05-06\t35714.58000000002\n2011-05-08\t18808.920000000046\n2011-05-09\t26060.43000000019\n2011-05-10\t45564.119999999915\n2011-05-11\t33240.36000000021\n2011-05-12\t59911.969999999856\n2011-05-13\t30744.07000000008\n2011-05-15\t9924.280000000004\n2011-05-16\t25279.77000000014\n2011-05-17\t53603.82999999985\n2011-05-18\t34337.29000000008\n2011-05-19\t34348.750000000015\n2011-05-20\t26256.520000000062\n2011-05-22\t24205.370000000068\n2011-05-23\t30739.550000000083\n2011-05-24\t37028.910000000054\n2011-05-25\t24152.280000000075\n2011-05-26\t33208.590000000026\n2011-05-27\t28232.19000000009\n2011-05-29\t7208.299999999988\n2011-05-31\t21967.96000000007\n2011-06-01\t20191.200000000124\n2011-06-02\t32502.010000000162\n2011-06-03\t16750.999999999996\n2011-06-05\t25520.35000000011\n2011-06-06\t16791.39\n2011-06-07\t37644.30000000004\n2011-06-08\t42940.909999999814\n2011-06-09\t45515.75000000025\n2011-06-10\t22540.659999999985\n2011-06-12\t12483.85999999999\n2011-06-13\t20372.930000000055\n2011-06-14\t40211.93000000002\n2011-06-15\t46139.179999999906\n2011-06-16\t34131.730000000054\n2011-06-17\t20800.72000000004\n2011-06-19\t22360.010000000104\n2011-06-20\t33493.4000000001\n2011-06-21\t22730.010000000064\n2011-06-22\t21794.940000000053\n2011-06-23\t24273.31000000014\n2011-06-24\t8619.880000000001\n2011-06-26\t6175.169999999984\n2011-06-27\t16823.859999999993\n2011-06-28\t34704.64000000011\n2011-06-29\t21775.43\n2011-06-30\t43834.550000000076\n2011-07-01\t13171.82000000001\n2011-07-03\t5977.139999999991\n2011-07-04\t44154.75000000023\n2011-07-05\t40334.9700000001\n2011-07-06\t26279.580000000133\n2011-07-07\t31357.72000000012\n2011-07-08\t26840.08000000012\n2011-07-10\t5692.069999999985\n2011-07-11\t22429.530000000123\n2011-07-12\t25892.040000000077\n2011-07-13\t11612.049999999948\n2011-07-14\t32575.960000000163\n2011-07-15\t14478.929999999933\n2011-07-17\t17174.660000000036\n2011-07-18\t28443.27000000024\n2011-07-19\t49316.780000000035\n2011-07-20\t27305.41000000014\n2011-07-21\t30957.06999999999\n2011-07-22\t20015.229999999978\n2011-07-24\t26476.200000000044\n2011-07-25\t26687.650000000176\n2011-07-26\t21271.30100000004\n2011-07-27\t25568.450000000106\n2011-07-28\t55706.880000000034\n2011-07-29\t18094.209999999955\n2011-07-31\t33486.359999999986\n2011-08-01\t21362.840000000022\n2011-08-02\t14947.269999999933\n2011-08-03\t27075.02000000017\n2011-08-04\t61028.6500000001\n2011-08-05\t21298.300000000094\n2011-08-07\t7464.1199999999935\n2011-08-08\t19987.149999999994\n2011-08-09\t26623.20000000008\n2011-08-10\t27474.220000000118\n2011-08-11\t72132.79000000005\n2011-08-12\t10049.47999999996\n2011-08-14\t5150.179999999998\n2011-08-15\t17205.54\n2011-08-16\t19103.710000000032\n2011-08-17\t49392.21999999998\n2011-08-18\t53225.669999999955\n2011-08-19\t17248.539999999994\n2011-08-21\t14549.210000000008\n2011-08-22\t27978.41000000003\n2011-08-23\t25756.30000000006\n2011-08-24\t37074.900000000045\n2011-08-25\t22458.880000000034\n2011-08-26\t25550.229999999978\n2011-08-28\t10784.779999999977\n2011-08-30\t31640.900000000096\n2011-08-31\t16117.999999999998\n2011-09-01\t37296.60000000006\n2011-09-02\t41745.07000000001\n2011-09-04\t17018.489999999998\n2011-09-05\t36844.04000000002\n2011-09-06\t28052.62000000006\n2011-09-07\t34125.65000000011\n2011-09-08\t26708.000000000106\n2011-09-09\t29317.690000000068\n2011-09-11\t35465.470000000096\n2011-09-12\t29039.310000000096\n2011-09-13\t54828.45000000007\n2011-09-14\t23360.660000000134\n2011-09-15\t62943.80999999994\n2011-09-16\t25858.060000000034\n2011-09-18\t15692.330000000029\n2011-09-19\t46212.21000000002\n2011-09-20\t109286.20999999993\n2011-09-21\t42944.07000000001\n2011-09-22\t57076.83000000002\n2011-09-23\t39426.48000000005\n2011-09-25\t31210.921000000104\n2011-09-26\t28642.27100000011\n2011-09-27\t35752.159999999916\n2011-09-28\t43383.03999999988\n2011-09-29\t43464.33000000008\n2011-09-30\t43992.84999999993\n2011-10-02\t11623.580000000014\n2011-10-03\t64214.78\n2011-10-04\t48240.839999999895\n2011-10-05\t75244.42999999986\n2011-10-06\t55306.27999999994\n2011-10-07\t47538.019999999815\n2011-10-09\t11922.239999999993\n2011-10-10\t44265.89000000007\n2011-10-11\t38267.75000000006\n2011-10-12\t29302.850000000173\n2011-10-13\t37067.17000000006\n2011-10-14\t35225.53999999997\n2011-10-16\t21605.440000000053\n2011-10-17\t47064.139999999956\n2011-10-18\t44637.84000000008\n2011-10-19\t36003.43000000002\n2011-10-20\t60793.13999999962\n2011-10-21\t62961.25999999998\n2011-10-23\t12302.410000000013\n2011-10-24\t38407.71999999994\n2011-10-25\t40807.49000000003\n2011-10-26\t37842.07999999995\n2011-10-27\t47480.1499999999\n2011-10-28\t39559.47000000007\n2011-10-30\t34545.28000000011\n2011-10-31\t48475.450000000164\n2011-11-01\t28741.55000000017\n2011-11-02\t45239.059999999816\n2011-11-03\t62816.54999999997\n2011-11-04\t60081.75999999989\n2011-11-06\t42912.400000000176\n2011-11-07\t70001.08000000009\n2011-11-08\t56647.659999999814\n2011-11-09\t62599.42999999966\n2011-11-10\t68956.24000000003\n2011-11-11\t54835.509999999995\n2011-11-13\t33520.22000000013\n2011-11-14\t112141.10999999996\n2011-11-15\t60594.229999999865\n2011-11-16\t64408.7000000001\n2011-11-17\t60329.719999999776\n2011-11-18\t48031.80000000006\n2011-11-20\t34902.01000000018\n2011-11-21\t48302.499999999796\n2011-11-22\t62307.31999999994\n2011-11-23\t78480.6999999997\n2011-11-24\t48080.279999999926\n2011-11-25\t50442.72000000002\n2011-11-27\t20571.500000000084\n2011-11-28\t55442.01999999994\n2011-11-29\t72219.19999999998\n2011-11-30\t59150.97999999988\n2011-12-01\t51410.94999999973\n2011-12-02\t57086.059999999874\n2011-12-04\t24565.78000000009\n2011-12-05\t57751.31999999973\n2011-12-06\t54228.37000000012\n2011-12-07\t75076.21999999967\n2011-12-08\t81417.77999999982\n2011-12-09\t32131.53000000001\n"},{"type":"TEXT","data":""}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=322"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462618_2013179384","id":"20190520-140933_785400989","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T05:51:58+0000","dateFinished":"2020-03-18T05:51:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"text":"//Weekly\n//write you DF solution here\n\n//So first we need to group by setting a 1 week window \n//Then need to get the sum of that week and output the start day and sum\nval weeklySum = retailCastDf.groupBy(window('InvoiceDate, \"1 week\").getField(\"start\").cast(\"date\").as(\"start\")).agg(sum('Quantity * 'UnitPrice).as(\"sum(amount)\")).orderBy('start)\n\n//Lets show the top 5 results\nweeklySum.show(5)\n\n//Now we need to create a view in order to pot the diagram\nweeklySum.createOrReplaceTempView(\"weeklySales\")","user":"anonymous","dateUpdated":"2020-03-18T05:53:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------------+\n|     start|       sum(amount)|\n+----------+------------------+\n|2010-11-25| 58635.56000000026|\n|2010-12-02| 266320.7600000051|\n|2010-12-09|234844.28000000227|\n|2010-12-16| 177360.1100000011|\n|2010-12-23|11796.310000000025|\n+----------+------------------+\nonly showing top 5 rows\n\nweeklySum: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: date, sum(amount): double]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=323"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462625_-1734340445","id":"20190520-140933_428817963","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T05:53:21+0000","dateFinished":"2020-03-18T05:53:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"text":"%sql\n-- Plot weekly diagram here\nselect start, `sum(amount)` from weeklySales","user":"anonymous","dateUpdated":"2020-03-18T05:53:48+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"to_date(weeklysales.`start`)":"string","sum(amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[],"groups":[],"values":[{"name":"sum(amount)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"start\tsum(amount)\n2010-11-25\t58635.56000000026\n2010-12-02\t266320.7600000051\n2010-12-09\t234844.28000000227\n2010-12-16\t177360.1100000011\n2010-12-23\t11796.310000000025\n2010-12-30\t13384.25000000006\n2011-01-06\t196304.23000000138\n2011-01-13\t148550.0200000006\n2011-01-20\t133280.75999999914\n2011-01-27\t117962.66999999882\n2011-02-03\t114742.56999999957\n2011-02-10\t127145.63999999926\n2011-02-17\t134762.3699999996\n2011-02-24\t115698.55999999898\n2011-03-03\t142363.89000000016\n2011-03-10\t119438.04999999919\n2011-03-17\t149267.0399999995\n2011-03-24\t197425.86000000124\n2011-03-31\t132980.15999999843\n2011-04-07\t122024.77999999902\n2011-04-14\t160589.66100000008\n2011-04-21\t87374.69999999974\n2011-04-28\t75286.71999999946\n2011-05-05\t188139.06000000125\n2011-05-12\t213801.21000000113\n2011-05-19\t176731.38000000015\n2011-05-26\t110808.24000000053\n2011-06-02\t172149.9600000006\n2011-06-09\t187264.30999999883\n2011-06-16\t155310.80999999994\n2011-06-23\t112372.29000000014\n2011-06-30\t173752.81000000166\n2011-07-07\t123823.48999999958\n2011-07-14\t169295.01000000126\n2011-07-21\t150975.90100000027\n2011-07-28\t170672.57999999894\n2011-08-04\t163875.63999999926\n2011-08-11\t173033.9199999991\n2011-08-18\t175833.02999999895\n2011-08-25\t106552.79000000161\n2011-09-01\t195082.47000000015\n2011-09-08\t198719.57999999844\n2011-09-15\t302936.69000000437\n2011-09-22\t235491.70199999926\n2011-09-29\t286780.81000000285\n2011-10-06\t226603.02999999942\n2011-10-13\t221603.56000000154\n2011-10-20\t253114.1000000017\n2011-10-27\t244040.9600000024\n2011-11-03\t355058.88000000786\n2011-11-10\t394456.01000001206\n2011-11-17\t332354.0500000061\n2011-11-24\t305906.7000000076\n2011-12-01\t320118.700000006\n2011-12-08\t113549.31\n"},{"type":"TEXT","data":""}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.fiery-return-259520.internal:4040/jobs/job?id=324"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1584481462626_-367281642","id":"20190520-212256_1274740776","dateCreated":"2020-03-17T21:44:22+0000","dateStarted":"2020-03-18T05:53:48+0000","dateFinished":"2020-03-18T05:53:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:81"}],"name":"Jarvis/2-DataFrame_pub","id":"2F3JDXJH2","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[],"hive:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}